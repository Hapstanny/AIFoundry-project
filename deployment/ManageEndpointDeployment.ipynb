{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LLM Application Deployment to **Azure AI Foundry** as Managed Endpoint with **Tracing & Evaluation**\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "* A basic understanding of AI Foundry SDK, Machine Learning SDK, and Large Language Models\n",
    "* An Azure Machine Learning Workspace and Azure Container Registry\n",
    "\n",
    "\n",
    "**Actions** - \n",
    "* Created a **python server API** based on Flask.\n",
    "* Deploy LLM Completion and evaluation & to an **AI Foundry Managed Online Endpoint**\n",
    "* Trace custom LLM application\n",
    "* Evaluate the results\n",
    "\n",
    "Managed online endpoints provide an easy to manage inferencing server for your ML workload. It's perfect for LLM based applications. Since we need a REST service, we won't use the default endpoint docker image, we will create a custom docker image instead.\n",
    "\n",
    "**Outline** - \n",
    "1. Prepare Dependencies\n",
    "2. Deploy to Managed Online Endpoint\n",
    "3. Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install required Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-identity\n",
    "pip install azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# from opentelemetry import trace\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "# from azure.search.documents import SearchClient\n",
    "# from config import ASSET_PATH, get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import (\n",
    "    MLClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set workspace details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"<Subscription ID>\"  # Azure Subscription ID\n",
    "RESOURCE_GROUP = \"<Resource Group>\"           # AI Foundry Resource Group\n",
    "AML_WORKSPACE_NAME = \"<AI Foundry Project>\"     # AI Foundry Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Login to your Azure account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    "    AzureCliCredential,\n",
    ")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential(additionally_allowed_tenants=[\"*\"])\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential(additionally_allowed_tenants=[\"*\"])\n",
    "\n",
    "# If login doesn't work above, uncomment the code below and login using device code\n",
    "# !az login --use-device-code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Container Registry and Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from azure.ai.ml import (\n",
    "    MLClient,\n",
    ")\n",
    "\n",
    "ml_client = MLClient(credential, SUBSCRIPTION_ID, RESOURCE_GROUP, AML_WORKSPACE_NAME)\n",
    "ws = ml_client.workspaces.get(AML_WORKSPACE_NAME)\n",
    "\n",
    "# Get the Azure Container Registry associated with the workspace\n",
    "acr = ws.container_registry\n",
    "\n",
    "# Parse the ACR resource Id for the ACR name\n",
    "match_object = re.match(r\".+?registries\\/(.+)\", acr)\n",
    "ACR_NAME = match_object.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the image in your ACR image\n",
    "ACR_IMAGE_NAME = \"serving\"\n",
    "\n",
    "!az acr build --image {ACR_IMAGE_NAME} --registry {ACR_NAME} ./environment/serving/. --resource-group {RESOURCE_GROUP}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Managed Online Endpoint\n",
    "### 2.1 Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a endpoint\n",
    "import datetime\n",
    "\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    ")\n",
    "\n",
    "from azure.ai.ml import (\n",
    "    MLClient,\n",
    ")\n",
    "\n",
    "time = str(datetime.datetime.now().strftime(\"%m%d%H%M%f\"))\n",
    "online_endpoint_name = f\"aml-llm-app1-{time}\"\n",
    "\n",
    "ml_client = MLClient(credential, SUBSCRIPTION_ID, RESOURCE_GROUP, AML_WORKSPACE_NAME)\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"online endpoint for LLM app\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deploy to the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineDeployment,\n",
    "    OnlineRequestSettings,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "deployment_name = f\"deploy-{time}\"\n",
    "sk_deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    model=Model(path=\"../aifoundrydemo\"),\n",
    "    request_settings=OnlineRequestSettings(request_timeout_ms=60000),\n",
    "    environment=Environment(\n",
    "        image=f\"{ACR_NAME}.azurecr.io/{ACR_IMAGE_NAME}:latest\",\n",
    "        name=\"serving\",\n",
    "        description=\"A generic serving environment, allowing customer to provide their own entry point to bring up an http server\",\n",
    "        inference_config={\n",
    "            \"liveness_route\": {\"port\": 5001, \"path\": \"/health\"},\n",
    "            \"readiness_route\": {\"port\": 5001, \"path\": \"/health\"},\n",
    "            \"scoring_route\": {\"port\": 5001, \"path\": \"/\"},\n",
    "        },\n",
    "    ),\n",
    "    environment_variables={\n",
    "        \"AZUREML_SERVING_ENTRYPOINT\": \"./aifoundrydemo/entry.sh\",\n",
    "        \"AIPROJECT_CONNECTION_STRING\": \"eastus.api.azureml.ms;<SUBSCRIPTION ID>;<RESOURCE GROUP>;<PROJECT NAME>\", # AI Project Connection String\n",
    "        \"AISEARCH_INDEX_NAME\": \"products-index\",\n",
    "        \"EMBEDDINGS_MODEL\": \"text-embedding-3-small\",\n",
    "        \"INTENT_MAPPING_MODEL\": \"gpt-4o-mini\",\n",
    "        \"CHAT_MODEL\": \"gpt-4o\",\n",
    "        \"EVALUATION_MODEL\": \"gpt-4o\",\n",
    "        \"APPLICATION_INSIGHTS_RESOURCE_ID\": f\"/subscriptions/<SUBSCRIPTION ID>/resourceGroups/<RESOURCE GROUP>/providers/Microsoft.Insights/components/<APP INSIGHT INSTANCE NAME>\", # Application Insights Resource ID\n",
    "        \"AOAI_CONNECTION_NAME\": f\"<AI CONNECTION NAME>\",  # AI Connection Name from AI Foundry\n",
    "        \"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\":True   # Enable content tracing\n",
    "    },\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    instance_type=\"Standard_F2s_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "ml_client.online_deployments.begin_create_or_update(sk_deployment).result()\n",
    "\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(endpoint.identity.principal_id)   # Get the principal ID of the endpoint to grant RBAC access to services like Key Vault, workspace storage, App insights etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test\n",
    "Now endpoint has been deployed, let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel musts:\n",
      " <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "url_parts = urlsplit(endpoint.scoring_uri)\n",
    "url = url_parts.scheme + \"://\" + url_parts.netloc\n",
    "\n",
    "token = ml_client.online_endpoints.get_keys(name=online_endpoint_name).primary_key\n",
    "headers = {\"Authorization\": \"Bearer \" + token, \"Content-Type\": \"application/json\"}\n",
    "payload = json.dumps(\n",
    "    {\n",
    "        \"query\": \"Items to carry for Everest expediation\",\"enable-telemetry\":True   # Pass the query to the endpoint and enable telemetry to APP Insights\n",
    "    }\n",
    ")\n",
    "\n",
    "response = requests.post(f\"{url}/chat/products\", headers=headers, data=payload)   # Call the endpoint to get LLM response\n",
    "print(f\"GenAI Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel musts:\n",
      " <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "url_parts = urlsplit(endpoint.scoring_uri)\n",
    "url = url_parts.scheme + \"://\" + url_parts.netloc\n",
    "\n",
    "token = ml_client.online_endpoints.get_keys(name=online_endpoint_name).primary_key\n",
    "headers = {\"Authorization\": \"Bearer \" + token, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(f\"{url}/chat/evaluations\", headers=headers)   # Genearete evaluation for the prior LLM conversation\n",
    "print(f\"Evaluation Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Clean up resources\n",
    "\n",
    "### 6.1 Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Delete the ACR Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr repository delete --name {ACR_NAME} --image {ACR_IMAGE_NAME}:latest --yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
